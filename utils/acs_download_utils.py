import os
import re
import json
import pandas as pd
from census import Census
from geoalchemy2 import WKTElement
from sqlalchemy import create_engine
from sqlalchemy.types import Float, Integer, Text
from geoalchemy2 import Geometry
from us import states 
from dotenv import load_dotenv 

# ========== å‚æ•°é…ç½® ==========
load_dotenv()
API_KEY = os.getenv("ACS_API_KEY")
YEAR = 2018
STATE_FIPS = '06'  # California
EXCEL_PATH = "H:/GoogleDrive/Dissertation/dissertation-local/dissertation-Paper-#3/acs_data_version#2.xlsx"
JSON_PATH = "../data/acs_variable_definitions.json"
ZIP_LATLON_PATH = "../data/us_zip_lat_lon.csv"


# ========== å‡½æ•°å®šä¹‰ ==========

def load_definition_table(json_path: str, excel_path: str) -> pd.DataFrame:
    """åŠ è½½å˜é‡å®šä¹‰è¡¨ï¼Œä¼˜å…ˆè¯»å– JSON ç¼“å­˜ï¼Œå¦åˆ™ä» Excel å¯¼å…¥å¹¶ä¿å­˜ JSONã€‚"""
    if os.path.exists(json_path):
        print(f"ğŸ” è¯»å–ç¼“å­˜ JSON æ–‡ä»¶: {json_path}")
        return pd.read_json(json_path)
    else:
        print(f"ğŸ“¥ è¯»å– Excel æ–‡ä»¶: {excel_path}")
        df_def = pd.read_excel(excel_path)
        df_def = df_def.dropna(subset=['Code'])
        df_def.to_json(json_path, orient="records", indent=2)
        print(f"âœ… å·²ç¼“å­˜ä¸º JSON æ–‡ä»¶: {json_path}")
    return df_def


def extract_variable_codes(code_series: pd.Series) -> list:
    """æå–å…¬å¼æˆ–ä»£ç ä¸­çš„æ‰€æœ‰åŸå§‹ ACS å˜é‡åã€‚"""
    all_vars = set()
    for formula in code_series.dropna():
        found = re.findall(r'[BC]\d{5}_\d+E', formula)
        all_vars.update(found)
    print(f"ğŸ” æå–å…¬å¼æˆ–ä»£ç ä¸­çš„æ‰€æœ‰åŸå§‹ ACS å˜é‡å")
    return sorted(all_vars)

def enrich_with_zip_info(df: pd.DataFrame, zip_info_path: str) -> pd.DataFrame:
    """
    æ¸…æ´— dfï¼Œå¹¶æ ¹æ® zip_info CSV æ·»åŠ  city å’Œ state ä¿¡æ¯ã€‚
    æ­¥éª¤ï¼š
    1. é‡å‘½å 'zip code tabulation area' ä¸º 'zipcode'
    2. å¦‚æœ 'state' åˆ—å­˜åœ¨ï¼Œåˆ™åˆ é™¤
    3. æ ¹æ® zipcode åˆå¹¶ zip_info ä¸­çš„ city å’Œ state
    """
    # é‡å‘½åå­—æ®µ
    if 'zip code tabulation area' in df.columns:
        df = df.rename(columns={'zip code tabulation area': 'zipcode'})

    # åˆ é™¤å·²æœ‰ state åˆ—
    if 'state' in df.columns:
        df = df.drop(columns=['state'])

    # åŠ è½½ ZIP -> city/state æ˜ å°„è¡¨
    zip_info = pd.read_csv(zip_info_path)
    zip_info.columns = zip_info.columns.str.lower()
    
    if not {'zip', 'county', 'city', 'state'}.issubset(zip_info.columns):
        raise ValueError("ZIP æ˜ å°„è¡¨å¿…é¡»åŒ…å« 'zip', 'city', 'state' å­—æ®µ")

    zip_info['zip'] = zip_info['zip'].astype(str)
    df['zipcode'] = df['zipcode'].astype(str)

    # åˆå¹¶
    df = df.merge(zip_info[['zip', 'county', 'city', 'state']], left_on='zipcode', right_on='zip', how='left')
    df.drop(columns=['zip'], inplace=True)

    return df



def download_acs_data(api_key: str, year: int, state_fips: str, var_codes: list) -> pd.DataFrame:
    """ä½¿ç”¨ Census API ä¸‹è½½æŒ‡å®šå˜é‡çš„ ACS æ•°æ®(ZIP çº§åˆ«)ã€‚"""
    c = Census(api_key)
    data = c.acs5.state_zipcode(
        fields=var_codes,
        state_fips=state_fips,
        zcta='*',
        year=year
    )
    df = pd.DataFrame(data)
    for var in var_codes:
        df[var] = pd.to_numeric(df[var], errors='coerce')

    return df

def download_acs_data_old(api_key: str, year: int, var_codes: list) -> pd.DataFrame:
    """
    ä¸‹è½½å…¨ç¾æ‰€æœ‰å·çš„ ACS ZIP çº§åˆ«æ•°æ®ï¼Œå¹¶åˆå¹¶ä¸ºä¸€ä¸ª DataFrameã€‚

    å‚æ•°ï¼š
        api_key: Census API Key
        year: å¹´ä»½ï¼ˆä¾‹å¦‚ 2018, 2019ï¼‰
        var_codes: è¦è¯·æ±‚çš„ ACS å˜é‡ä»£ç åˆ—è¡¨

    è¿”å›ï¼š
        åŒ…å«å…¨ç¾ ZIP æ•°æ®çš„ Pandas DataFrame
    """
    c = Census(api_key)
    all_dfs = []

    # æ’é™¤é˜¿æ‹‰æ–¯åŠ ã€å¤å¨å¤·ã€åç››é¡¿ç‰¹åŒºã€æµ·å¤–é¢†åœ°ç­‰
    excluded = {"AK", "HI", "DC", "PR", "VI", "GU", "MP", "AS"}

    # è·å–å¤§é™†å·çš„ State å¯¹è±¡åˆ—è¡¨
    continental_states = [s for s in states.STATES if s.abbr not in excluded]

    for state in states.STATES:
        state_fips = state.fips
        try:
            print(f"â¬‡ æ­£åœ¨ä¸‹è½½ {state.name} ({state_fips}) çš„æ•°æ®...")
            data = c.acs5.state_zipcode(
                fields=var_codes,
                state_fips=state_fips,
                zcta='*',
                year=year
            )
            df = pd.DataFrame(data)
            for var in var_codes:
                df[var] = pd.to_numeric(df[var], errors='coerce')
            all_dfs.append(df)
        except Exception as e:
            print(f"âŒ æ— æ³•ä¸‹è½½ {state.name}ï¼ˆ{state_fips}ï¼‰: {e}")
            continue

    # åˆå¹¶æ‰€æœ‰å·çš„æ•°æ®
    if all_dfs:
        df_all = pd.concat(all_dfs, ignore_index=True)
        return df_all
    else:
        print("âš ï¸ æœªèƒ½æˆåŠŸä¸‹è½½ä»»ä½•å·çš„æ•°æ®ã€‚")
        return pd.DataFrame()


def calculate_custom_variables(df_raw: pd.DataFrame, df_def: pd.DataFrame) -> pd.DataFrame:
    """æ ¹æ®è‡ªå®šä¹‰å…¬å¼è®¡ç®—è¡ç”Ÿå˜é‡ã€‚"""
    def get_vars_from_formula(formula):
        return re.findall(r'[BC]\d{5}_\d+E', formula)

    def safe_eval_formula(df, formula, var_name):
        needed_vars = get_vars_from_formula(formula)
        missing = [v for v in needed_vars if v not in df.columns]
        if missing:
            print(f"âš ï¸ è·³è¿‡ {var_name}ï¼Œç¼ºå¤±å˜é‡: {missing}")
            return None
        try:
            return df.eval(formula)
        except Exception as e:
            print(f"âŒ é”™è¯¯å…¬å¼ {var_name}: {e}")
            return None

    df = df_raw.copy()
    for _, row in df_def.iterrows():
        var_name = row['Definition']
        formula = row['Code']
        result = safe_eval_formula(df, formula, var_name)
        if result is not None:
            df[var_name] = result
    print(f"ğŸ” æ ¹æ®è‡ªå®šä¹‰å…¬å¼è®¡ç®—è¡ç”Ÿå˜é‡")
    return df


def clean_and_rename_columns(df: pd.DataFrame, df_def: pd.DataFrame) -> pd.DataFrame:
    """é‡å‘½ååˆ—å¹¶æ ‡å‡†åŒ–å­—æ®µæ ¼å¼ã€‚"""
    geo_cols = ['state', 'county', 'city', 'zipcode']
    custom_vars = df_def['Definition'].tolist()
    df = df[geo_cols + custom_vars]
    # df = df.rename(columns={'zip code tabulation area': 'zipcode'})
    df.columns = (
        df.columns
        .str.strip()
        .str.lower()
        .str.replace('%', 'pct', regex=False)
        .str.replace(' ', '_')
        .str.replace('-', '_')
    )
    print(f"ğŸ” é‡å‘½ååˆ—å¹¶æ ‡å‡†åŒ–å­—æ®µæ ¼å¼")
    return df


def attach_zip_latlon_geom(df: pd.DataFrame, zip_latlon_csv_path: str) -> pd.DataFrame:
    """åˆå¹¶ ZIP ç»çº¬åº¦åæ ‡ï¼Œå¹¶åˆ›å»º PostGIS å…¼å®¹çš„ geom åˆ—ã€‚"""
    zip_coords = pd.read_csv(zip_latlon_csv_path)
    zip_coords.columns = zip_coords.columns.str.lower()

    if not {'zip', 'lat', 'lng'}.issubset(zip_coords.columns):
        raise ValueError("CSV æ–‡ä»¶å¿…é¡»åŒ…å« 'zip', 'lat', 'lng' åˆ—")

    zip_coords['zip'] = zip_coords['zip'].astype(str)
    df['zipcode'] = df['zipcode'].astype(str)

    df = df.merge(zip_coords[['zip', 'lat', 'lng']], left_on='zipcode', right_on='zip', how='left')
    df.drop(columns=['zip'], inplace=True)

    df['geom'] = df.apply(
        lambda row: WKTElement(f"POINT({row['lng']} {row['lat']})", srid=4326)
        if pd.notnull(row['lat']) and pd.notnull(row['lng']) else None,
        axis=1
    )
    return df

def write_df_to_postgres(df, table_name, db_url):
    """
    å°† DataFrame å†™å…¥ PostgreSQL è¡¨ï¼ˆæ”¯æŒ PostGIS geom åˆ—ï¼‰

    å‚æ•°:
        df: åŒ…å«æ•°æ®çš„ DataFrameï¼Œå¿…é¡»åŒ…å« 'geom' åˆ—ï¼ˆGeoAlchemy WKTElement ç±»å‹ï¼‰
        table_name: ç›®æ ‡è¡¨åï¼Œä¾‹å¦‚ 'acs_data'
        db_url: SQLAlchemy æ ¼å¼çš„è¿æ¥å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ 'postgresql+psycopg2://user:pwd@localhost:5432/db'
    """
    engine = create_engine(db_url)

    # å­—æ®µç±»å‹æ˜ å°„ï¼ˆå¯ä»¥æ ¹æ®å®é™…æ•°æ®å†ç»†åŒ–ï¼‰
    dtype = {
        'zipcode': Text(),
        'state': Text(),
        'county': Text(),
        'city': Text(),
        'year': Integer(),
        'population': Integer(),
        'median_income': Float(),
        'per_capita_income': Float(),
        'geom': Geometry('POINT', srid=4326)
    }

    # è‡ªåŠ¨è¯†åˆ«å…¶ä½™åˆ—ä¸º Float ç±»å‹
    for col in df.columns:
        if col not in dtype and col not in ['geom']:
            dtype[col] = Float()

    # å†™å…¥æ•°æ®åº“
    df.to_sql(
        name=table_name,
        con=engine,
        if_exists='append',
        index=False,
        dtype=dtype,
        method='multi'
    )

    print(f"âœ… æ•°æ®å·²æˆåŠŸå†™å…¥ PostgreSQL è¡¨ `{table_name}`")


# ========== ä¸»æµç¨‹ ==========
def main(year = 2016):
    # 1. åŠ è½½å˜é‡å®šä¹‰
    df_def = load_definition_table(JSON_PATH, EXCEL_PATH)

    # 2. æå–åŸå§‹å˜é‡ä»£ç 
    var_codes = extract_variable_codes(df_def['Code'])

    # 3. ä¸‹è½½åŸå§‹ ACS æ•°æ®
    if year >= 2020:
        df_raw = download_acs_data(API_KEY, year, STATE_FIPS, var_codes)
    else:
        df_raw = download_acs_data_old(API_KEY, year, var_codes)

    df_enriched = enrich_with_zip_info(df_raw, "../data/us_zip_lat_lon.csv")
    # 4. è®¡ç®—è‡ªå®šä¹‰æŒ‡æ ‡
    df_final = calculate_custom_variables(df_enriched, df_def)
    # 5. é‡å‘½åã€æ¸…æ´—åˆ—
    df_cleaned = clean_and_rename_columns(df_final, df_def)

    # 6. é™„åŠ ç»çº¬åº¦ä¸ç©ºé—´å­—æ®µ
    df_with_geom = attach_zip_latlon_geom(df_cleaned, ZIP_LATLON_PATH)

    # 7. æ·»åŠ å¹´ä»½ä¿¡æ¯
    df_with_geom['year'] = year

    # 8. ä¿å­˜ç»“æœ
    db_url = "postgresql+psycopg2://postgres:wym45123@localhost:5432/dashboard"
    write_df_to_postgres(df_with_geom, table_name="acs_data_all", db_url=db_url)


if __name__ == "__main__":
    main()